\documentclass{article}
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL nips_gary.tex                 Fri May 19 08:50:05 2017
%DIF ADD ../NIPS2017/nips_2017-1.tex   Fri May 19 11:59:27 2017

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage[hidelinks]{hyperref}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{graphicx}
%\usepackage{subfigure}

\newenvironment{mycell}[1]
{
	\begin{minipage}{#1}
		\begin{center}
			\vspace*{0.15cm}
		}
		{
			\vspace*{0.1cm}
		\end{center}
	\end{minipage}
}

\title{Generalised Training of Spiking Neural Networks}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
	David S.~Hippocampus\thanks{Use footnote for providing further
		information about author (webpage, alternative
		address)---\emph{not} for acknowledging funding agencies.} \\
	Department of Computer Science\\
	Cranberry-Lemon University\\
	Pittsburgh, PA 15213 \\
	\texttt{hippo@cs.cranberry-lemon.edu} \\
	%% examples of more authors
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}
	% \nipsfinalcopy is no longer used

	\maketitle

	\begin{abstract}
		%	To train time-based, biologically-plausible spiking neural networks~(SNNs) on conventional artificial neural networks~(ANNs), 
		Spiking neural networks~(SNNs) can be trained by first training an equivalent ANN and then transferring the trained weights to the SNN, but there are two significant problems to be solved.
		First, an accurate activation function is needed to model the neural dynamics of spiking neurons, and our previously proposed activation function, Noisy Softplus, has shown to be a good match to the response activity of Leaky Integrate-and-Fire~(LIF) neurons.
		The second problem is mapping the abstract numerical values of the ANN to concrete physical units in the SNN, such as current and firing rate.
		In this paper, we introduce the parametric activation function~(PAF)\DIFaddbegin \DIFadd{, $p \times f(x)$, }\DIFaddend to tackle the second problem.
%DIF > 		With these problems solved, SNNs can be trained exactly the same way as ANNs, and the trained weights can be used directly in the spiking version of the same network without any conversion.
%DIF > 		More importantly, the PAF can be generalised to activation functions other than Noisy Softplus, such as the Rectified Linear Unit~(ReLU).%, and the parameters are independent to activation functions.
		With these problems solved, \DIFdelbegin \DIFdel{SNNs can be trained exactly the same way asANNs, and the }\DIFdelend \DIFaddbegin \DIFadd{SNN training can be simplified as: (1) estimate parameter $p$ for PAF according to biological configurations of LIF neurons, and (2) use PAF instead of conventional activation functions to train an equivalent ANN.
		The }\DIFaddend trained weights can be \DIFdelbegin \DIFdel{used }\DIFdelend \DIFaddbegin \DIFadd{transferred }\DIFaddend directly in the spiking version of the same network without any conversion.
		\DIFdelbegin \DIFdel{More importantly, the PAF can be generalised to activation functions other than Noisy Softplus, such as the Rectified Linear Unit~(ReLU). %DIF < , and the parameters are independent to activation functions.
    Therefore, SNN training can be simplified as: estimate the parameters for PAF according to biological configurations of LIF neurons and use a PAF-based
    %DIF < parametric ReLU as the 
    activation function when training.
    }\DIFdelend In addition, we propose a fine tuning method as an \DIFdelbegin \DIFdel{optional step }\DIFdelend \DIFaddbegin \DIFadd{option }\DIFaddend which helps the trained network to match the SNN more closely.
		Based on this generalised training method, we achieve the best SNN accuracy on the MNIST task using LIF neurons, 98.85\%, on a 6-layer spiking convolutional neural network~(ConvNet).

		
		
		%  We extended the work of proposed activation function, Noisy Softplus, to fit into training of layered up deep spiking neural networks~(SNNs).
		%  Thus, a time-dependant, biologically-plausible spiking neuron can be modelled as a traditional rate-based neuron in artificial neural networks~(ANNs).
		%  Such an ANN can be trained simply by the traditional algorithm, for example Back Propagation~(BP), and the trained weights can be directly used in the spiking version of the same network without any conversion.
		%  Furthermore, the training method has been generalised to other activation units, for instance Rectified Linear Units (ReLU), to train deep SNNs off-line.
		%  This research is crucial to provide a generalised SNN training method, to improve the learning ability of SNNs with biological characteristics, and to close the gap towards equivalent performance as ANNs.

	\end{abstract}

	\section{Introduction}
	Advances in computing power and deep learning have benefited computers with a rapidly growing performance in cognitive tasks, such as recognising objects~\cite{deng2009imagenet} \DIFdelbegin \DIFdel{or }\DIFdelend \DIFaddbegin \DIFadd{and }\DIFaddend playing GO~\cite{silver2016mastering}. 
	These tasks were once dominated by human intelligence and solved by biological neurons in the brain.
	However, humans and many other animals still win against computers in practical tasks and outperform in the terms of size and energy by several orders of magnitude.
	For instance, AlphaGO~\cite{silver2016mastering} consumed 1~MW of power on its 1920 CPUs and 280 GPUs when playing the game with one of the best human players whose brain merely consumed about 20~W.
	Although we are still far from understanding the brain thoroughly, it is believed that the performance gap between computation in the biological nervous system and in a computer lies in the fundamental computing units and the way they act.
	Computers employ Boolean logic and deterministic digital operations based usually on synchronous clocks while nervous systems employ parallel-distributed, event-driven, \DIFdelbegin \DIFdel{stochastic, }\DIFdelend \DIFaddbegin \DIFadd{stochastically }\DIFaddend unreliable components~\cite{indiveri2009artificial}\DIFaddbegin \DIFadd{, neurons}\DIFaddend .
	The impressive disparities in cognitive capabilities and energy consumption drives the research into biologically-plausible spiking neurons \DIFaddbegin \DIFadd{and brain inspired computers, known as neuromorphic engineering~\mbox{%DIFAUXCMD
\cite{furber2016bio}
}%DIFAUXCMD
}\DIFaddend .

	A regular artificial neuron (Figure~\ref{Fig:compare_as}\DIFdelbegin \DIFdel{a}\DIFdelend \DIFaddbegin \DIFadd{(a)}\DIFaddend ) comprises a weighted summation of input data, $\sum x_i w_i$, and an activation function, $f$, applied on the sum.
	Usually, a bias is included in the weighted summation which can be seen as an extra input $x_b = 1$ with its weight set to $b$.
	However, in this paper we exclude biases for both artificial and spiking neurons to simplify neural models and to reduce the number of parameters.
%DIF > 	Meanwhile the inputs of a spiking neuron (Figure~\ref{Fig:compare_as}(b)) are pre-synaptic spike trains, which create post-synaptic potentials~(PSPs) and trigger spikes as outcomes when the neuron's membrane potential reaches some threshold.
	Meanwhile the inputs of a spiking neuron (Figure~\ref{Fig:compare_as}\DIFdelbegin \DIFdel{b) are pre-synaptic }\DIFdelend \DIFaddbegin \DIFadd{(b)) are }\DIFaddend spike trains, which \DIFdelbegin \DIFdel{create Post-Synaptic Potentials}\DIFdelend \DIFaddbegin \DIFadd{generates current influx through neural synapses (connections).
	A single spike creates a current pulse with an amplitude of $w$, which is defined as the synaptic efficacy, and the current then decays exponentially with a decaying rate determined by the synaptic time constant, $\tau_{syn}$.
	The current pulses then consequently produce post-synaptic potentials}\DIFaddend ~(PSPs) \DIFaddbegin \DIFadd{on the neuron driving its membrane potential change over time, }\DIFaddend and trigger spikes as outcomes when the neuron's membrane potential reaches some threshold.
	The dynamics of the \DIFdelbegin \DIFdel{membrane potentials}\DIFdelend \DIFaddbegin \DIFadd{current influxes}\DIFaddend , PSPs, \DIFaddbegin \DIFadd{membrane potentials, }\DIFaddend and spike trains are all time dependent, while neurons of ANNs \DIFdelbegin \DIFdel{(e.g. sigmoid units) }\DIFdelend only cope with abstract numerical values representing spiking rate, without timing information.
	These fundamental differences in input/output representation and neural computation raise the research problem of how to \DIFdelbegin \DIFdel{train and operate }\DIFdelend \DIFaddbegin \DIFadd{operate and train }\DIFaddend biologically-plausible SNNs to \DIFdelbegin \DIFdel{be competent }\DIFdelend \DIFaddbegin \DIFadd{make them as competent as ANNs }\DIFaddend in cognitive tasks\DIFdelbegin \DIFdel{just as ANNs are}\DIFdelend .

	\begin{figure}[tb!]
		\centering
		\begin{subfigure}[t]{0.32\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/neuron_ann.pdf}
			%			\caption{Current sampled at $dt$=1~ms.}
		\end{subfigure}~
		\begin{subfigure}[t]{0.65\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/neuron_snn.pdf}
			%			\caption{Current sampled at $dt$=10~ms.}
		\end{subfigure}
		\caption{Comparison of the processing mechanisms of an artificial and a spiking neuron. (a) An artificial neuron takes numerical values of vector \textbf{x} as input, performs a weighted summation followed by an activation function $f$. (b) Spike trains flow into a spiking neuron as \DIFdelbeginFL \DIFdelFL{input stimuli}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{current influx}\DIFaddendFL , trigger linearly summed PSPs through synapses with different synaptic efficacy \textbf{w}, and the post-synaptic neuron generates output spikes when the membrane potential reaches some threshold.}
		\label{Fig:compare_as}
	\end{figure}

	An intuitive idea is to train SNNs on an equivalent ANN and then transfer the trained weights to the SNN.
	Jug et al.~\cite{Jug_etal_2012} first proposed the use of the Siegert formula~\cite{siegert1951first} as the activation function in training Deep Belief Networks, which maps incoming currents driven by Poisson spike trains \DIFdelbegin \texttt{}%DIFAUXCMD
\DIFdelend to the response firing rate of an LIF neuron.
	The \DIFdelbegin \DIFdel{activation function's variables have }\DIFdelend \DIFaddbegin \DIFadd{variables of the activation function are in }\DIFaddend physical units, thus the trained weights can be transferred directly into SNNs.
	%Jug et al\cite{Jug_etal_2012} first proposed the use of the Siegert function to replace the sigmoid activation function in training Restricted Boltzmann Machines (RBMs).
	%The Siegert units map incoming currents driven by Poisson spike trains to the response firing rate of an LIF neuron.
	%The ratio of the spiking rate to its maximum is equivalent to the output of a sigmoid neuron.
	%A spiking Deep Belief Network (DBN)~\cite{Stromatias2015scalable} of four layers of RBMs was implemented on neuromorphic hardware, SpiNNaker~\cite{furber2014spinnaker}, to recognise hand written digits in real time.
	However, \DIFdelbegin \DIFdel{most importantly, }\DIFdelend the Siegert formula is inaccurate as it models \DIFaddbegin \DIFadd{the }\DIFaddend current noise as white\DIFdelbegin \DIFdel{, but taking no notice of the coloured noise generated by the synaptic time constant $\tau_{syn}$ of spike arrivals. The only scenario in which current noise is white arises when $\tau_{syn} \to 0$.
}\DIFdelend \DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{liu2016noisy}
}%DIFAUXCMD
, $\tau_{syn} \to 0$, but it is not feasible in practice.
%DIF > 	taking no notice of the coloured noise generated by the synaptic time constant $\tau_{syn}$ of spike arrivals, since the current noise is only a white noise when $\tau_{syn} \to 0$~.
	}\DIFaddend Moreover, the high complexity of the Siegert function \DIFdelbegin \DIFdel{causes longer training time and energy consumption, as well as the high-cost }\DIFdelend \DIFaddbegin \DIFadd{and the }\DIFaddend computation of its derivative to obtain the error gradient \DIFaddbegin \DIFadd{cause much longer training time and more energy, comparing to regular ANN activation functions, such as Sigmoid}\DIFaddend .
	%Additionally, neurons have to fire at high frequency (higher than half of the maximum firing rate) to represent the activation of a sigmoid unit; thus the network activity results in high power dissipation.
	\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend %Hence researchers turned to abstract activation functions to model the response activity of spiking neurons to simplify the computation. %and at the same time improve the model accuracy of the activation function.
\DIFdelbegin \DIFdel{Hunsberger and Eliasmith developed an activation function which is a }\DIFdelend %DIF > 	A similar activation function of Soft LIF~\cite{hunsberger2015spiking} was introduced to simplify the computation complexity.
	\DIFaddbegin \DIFadd{A }\DIFaddend softened version of the response function of LIF neurons \DIFaddbegin \DIFadd{is proposed~\mbox{%DIFAUXCMD
\cite{hunsberger2015spiking}
}%DIFAUXCMD
}\DIFaddend and is less computationally expensive than the Siegert function\DIFdelbegin \DIFdel{~\mbox{%DIFAUXCMD
\cite{hunsberger2015spiking}
}%DIFAUXCMD
}\DIFdelend .
	However, the model ignored the \DIFdelbegin \DIFdel{noise }\DIFdelend \DIFaddbegin \DIFadd{dynamic noise change }\DIFaddend introduced by input spikes, assuming static \DIFdelbegin \DIFdel{current influx into neurons}\DIFdelend \DIFaddbegin \DIFadd{noise level of the current influx}\DIFaddend .
	Therefore the training required additional noise on the response firing rate and on the training data, thus included hyper-parameters in the model.
\DIFdelbegin \DIFdel{What's more, the activation function compromised the modelling accuracy for computational simplicity.
}\DIFdelend %DIF > 	What's more, the activation function compromised the modelling accuracy for computational simplicity.

	
	\DIFdelbegin \DIFdel{Even though these activation functions have been successfully used to train ANNs and transfer their weights to SNNs, they do not solve the problem of accurately modelling }\DIFdelend \DIFaddbegin \DIFadd{Therefore, the first problem is the accurate modelling of }\DIFaddend the neural response activity of LIF neurons \DIFdelbegin \DIFdel{. %DIF <  with abstract activation functions.
Noisy Softplus~\mbox{%DIFAUXCMD
\cite{liu2016noisy}
}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{using abstract activation functions, in the hope of (1) increasing the performance of SNN training and (2) reducing the computation complexity.
	We call these activation functions `abstract' referring to the ones without physical units which are used in the ANNs, and select them for LIF modelling due to their simplicity and generalised training in ANNs.
	Noisy softplus~\mbox{%DIFAUXCMD
\cite{liu2016noisy}
}%DIFAUXCMD
, }\DIFaddend proved to be a close match to the response activity of LIF neurons by including \DIFaddbegin \DIFadd{current }\DIFaddend noise as the second factor in the activation function and taking account of coloured noise driven by $\tau_{syn}$.

	
	\DIFdelbegin \DIFdel{We call activation functions `abstract' to those whose variables operate without physical units and are used in ANNs, we chose them for LIF modelling due to their simplicity and generalised training in ANNs. 
We now study }\DIFdelend \DIFaddbegin \DIFadd{Then }\DIFaddend the second problem \DIFdelbegin \DIFdel{, which is how to map }\DIFdelend \DIFaddbegin \DIFadd{is to map the }\DIFaddend abstract activation functions to physical units: current in \textit{nA} and firing rates in \textit{Hz}.
	%Better learning performance has been reported using ReLU, so modelling ReLU-like activation function for spiking neurons is needed.  
	%Based on the fact that cortical neurons seldom saturate their firing rate as sigmoid neurons,ReLU~\cite{glorot2011deep} were proposed to replace sigmoid neurons and surpassed the performance of other popular activation units.
	%trained with noisy input, the 4-layered spiking autoencoder reached 98.37\% accuracy on MNIST. 
	Instead of solving the problem, an alternative way of converting ANN-trained weights for use in SNNs~\cite{cao2015spiking,diehl2015fast} was successfully applied 
	on less biologically-realistic and simplified integrate-and-fire (IF) neurons.
	\DIFdelbegin \DIFdel{These alternative methods normalise }\DIFdelend \DIFaddbegin \DIFadd{Normalising }\DIFaddend the ReLU-trained weights for use on simplified IF neurons \DIFdelbegin \DIFdel{, which is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend relatively straightforward, so this method sets the state-of-the-art performance.
	However, \DIFdelbegin \DIFdel{in this paper we aim }\DIFdelend \DIFaddbegin \DIFadd{this paper aims }\DIFaddend to address the \DIFaddbegin \DIFadd{second }\DIFaddend problem of mapping activation functions to the response firing activity of biologically-plausible LIF neurons\DIFdelbegin \DIFdel{and, thus , to complete the generalised }\DIFdelend \DIFaddbegin \DIFadd{, thus to complete a formalised }\DIFaddend SNN training mechanism \DIFaddbegin \DIFadd{and to generalise the method to commonly-used simple activation functions, e.g. ReLU}\DIFaddend .

	
	%Recent developments on ANN-trained SNN models has focused on using ReLU units and converting trained weights to fit in SNNs.
	%Better performance~\cite{cao2015spiking,diehl2015fast} than Siegert-trained RBM has been demonstrated in spiking ConvNets.
	%But this training method employed less biologically-realistic and simplified integrate-and-fire (IF) neurons.
	%The training used only ReLUs and zero bias to avoid negative outputs, and applied a deep learning technique, dropout\cite{srivastava2014dropout} , to increase the classification accuracy.

	%This work was extended to a Recursive Neural Network (RNN)~\cite{diehl2016conversion} and run on the TrueNorth\cite{merolla2014million} neuromorphic hardware platform.

	%Except for the popular, simplified version of ReLU, $max(0,\sum w x)$, the other implementation of $\log(1+e^x)$, ``Softplus'', is more biologically realistic.
	%Recent work~\cite{hunsberger2015spiking} proposed the Soft LIF response function for training SNNs, which is equivalent to Softplus activation of ANNs.
	%Furthermore, neuroscientific study has showed that the spike train of individual neurons is far from being periodic, which thus brings noisy to the input signal of spiking neurons~\cite{Gerstner:2002}.

	
	%Therefore, in the previous work of Qian Liu et al.~\cite{liu2016noisy}, the difference between analytical estimation and practical simulations of spiking neurons were compared, and a new activation function named Noisy Softplus was proposed to match the response function of LIF neurons. In order to close the gap between the performance of SNNs and ANNs, and to further improve the performance of SNNs, we extended Noisy Softplus with a scale factor and proposed a complete layered up SNN training method by using artificial neurons of combined activation.

	\DIFdelbegin \DIFdel{We present an SNN training methodology via direct weight transfer from PAF-trained ANNs. Our method is compatible with existing ANN training methods, for changing abstract activation functions to their PAF version merely requires a linear scaling. The scale factor can be easily acquired by well-known and widely-available estimation techniques (e.g. curve fitting) from simple SNN simulations. Furthermore, once an LIF neuron is characterized, the scaling parameter can be used for any SNN which uses  LIF neurons with the same attributes. Training compatibility allows us to choose computationally simple activation functions to increase training speed. Surprisingly, the use of PAF-ReLU provides the best accuracy scores for an SNN on the MNIST dataset.
}\DIFdelend This paper will start with a brief review on modelling the LIF response function with Noisy Softplus in Section~\ref{sec:back}, and introduce the PAF in Section~\ref{sec:meth} to address the second problem mentioned above and complete the generalised SNN training method.
	In Section~\ref{sec:result} we will demonstrate the training of a spiking ConvNet, and compare the proposed method to existing training algorithms.

	\section{Background}
	\label{sec:back}
	\begin{figure}[thb!]
		\centering
		\begin{subfigure}[t]{0.4\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/siegert.png}
			\caption{\DIFdelbeginFL \DIFdelFL{Noisy Softplus}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Response firing rate of an LIF neuron}\DIFaddendFL }
		\DIFdelbeginFL %DIFDELCMD < \label{fig:ns-behaviour}
%DIFDELCMD < 	%%%
\DIFdelendFL \end{subfigure}~~~~~~
		\begin{subfigure}[t]{0.4\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/4.pdf}
			\caption{Noisy Softplus\DIFdelbeginFL \DIFdelFL{in 3D}\DIFdelendFL }
		\DIFdelbeginFL %DIFDELCMD < \label{fig:ns-fixed-k}
%DIFDELCMD < 	%%%
\DIFdelendFL \end{subfigure}
		\caption{
			%		Noisy Softplus fits to the response function of the LIF neuron.
			Noisy Softplus models the LIF response function.
			(a) Actual firing rates measured by simulations on an LIF neuron driven by different input currents and discrete noise levels.
			Bold lines show the average and the grey colour fills the range between the minimum and the maximum.
			(b) Noisy Softplus activates the input $x$ according to different noise levels where $k=0.16$.}
		\label{fig:ns}
	\end{figure}
	To model the response function of LIF neurons (see Figure~\DIFdelbegin \DIFdel{\ref{fig:ns-behaviour}}\DIFdelend \DIFaddbegin \DIFadd{\ref{fig:ns}(a)}\DIFaddend ) whose output firing rates are determined by the mean and variance of the noisy input currents, we proposed the Noisy Softplus:
	\begin{equation}
	y = f_{ns}(x, \sigma) = k \sigma \log [1 + \exp(\frac{x}{k \sigma})]~,
	\label{equ:nsp}
	\end{equation}
	where $x$ and $\sigma$ refer to the mean and standard deviation of the input current, $y$ indicates the intensity of the output firing rate, and $k$, determined by the biological configurations on the LIF neurons~(listed in Table~\ref{tbl:pynnConfig}), controls the shape of the curves.
	Note that the novel activation function we proposed contains two parameters, the mean current and its noise, which can be estimated by:
	\begin{equation}
	x = \tau_{syn}\sum_i w_i\lambda_{i}~, ~\sigma^2=\frac{1}{2}\tau_{syn}\sum_i w_i^2\lambda_{i}~,
	\label{equ:distr}
	\end{equation}
	where $\lambda_i$ indicates the firing rate of an input spike train.
	%; both are naturally obtained in spiking neurons.
	% With doubled information, more powerful training methods and network models are expected. 
	Figure~\DIFdelbegin \DIFdel{\ref{fig:ns-fixed-k} }\DIFdelend \DIFaddbegin \DIFadd{\ref{fig:ns}(b) }\DIFaddend shows the activation function in curve sets corresponding to different discrete noise levels which \DIFdelbegin \DIFdel{mimic }\DIFdelend \DIFaddbegin \DIFadd{mimics }\DIFaddend the responding activities of LIF neurons.
	The derivative of the Noisy Softplus is the logistic function scaled by $k\sigma$:
	\begin{equation}
	\frac{\partial f_{ns}(x,\sigma)}{\partial x} = \frac{1}{1+exp(-\frac{x}{k\sigma})}~~,
	\label{equ:logist}
	\end{equation}	
	which could \DIFdelbegin \DIFdel{be applied easily }\DIFdelend \DIFaddbegin \DIFadd{easily be applied }\DIFaddend to back propagation in any \DIFdelbegin \DIFdel{network }\DIFdelend \DIFaddbegin \DIFadd{ANN }\DIFaddend training.

	\begin{table}[thb]
		\centering
		\caption{\label{tbl:pynnConfig}Default parameter settings for the current-based LIF neurons used through this paper, for PyNN~\cite{davison2008pynn} simulations.}
		\bgroup
		\def\arraystretch{1.4}
		\begin{tabular}{c c c c c c c}
			%\hline
			cm & tau\_m & tau\_refrac & v\_reset & v\_rest& v\_thresh & i\_offset \\
			\hline
			0.25 nF & 20.0 ms & 1.0 ms & -65.0 mV & -65.0 mV & -50.0 mV &  0.1 nA 
		\end{tabular}
		\egroup
	\end{table}

	
	\section{Methods}	
	\label{sec:meth}

	\subsection{Mapping Noisy Softplus to Concrete Physical Units}
	\label{sec:af_model}
	The inputs of the Noisy Softplus, $x$ and $\sigma$, are obtained from physical variables as stated in Equation~\ref{equ:distr}, thus they are already in physical units (\textit{nA}).
	Therefore, linearly scaling up the activation function by a factor~$S$~(\textit{Hz}/\textit{nA}) can approximate the output firing rate $\lambda_{out}$ of an LIF neuron in \textit{Hz}:
	\begin{equation}
	\lambda_{out} \simeq f_{ns}(x, \sigma) \times S = k \sigma \log [1 + \exp(\frac{x}{k \sigma})] \times S~.
	\label{equ:fit}
	\end{equation}	

	
	Suitable calibrations of $k$ and $S$ in Equation~\ref{equ:fit} enables Noisy Softplus to closely match the practical response firing rates of LIF neurons given various biological parameters.
	The parameter pair of $(k, S)$ is curve-fitted with the triple data points of $(\lambda_{out}, x, \sigma)$ and the calibration currently is conducted by linear least squares regression.
	The output firing rate $\lambda_{out}$ is measured from SNN simulations where an LIF neuron is driven by synaptic input current of Poisson spike trains.
	Figure~\ref{Fig:nsptau1} shows two calibration results \DIFdelbegin \DIFdel{in which 
}\DIFdelend \DIFaddbegin \DIFadd{that 
	}\DIFaddend the parameters were fitted to $(k, S)=(0.19,208.76)$ when the synaptic constant is set to $\tau_{syn}=1$~ms and was fitted to $(k, S)=(0.35,201.06)$ when $\tau_{syn}=10$~ms.
	%numerical analysis is considered however for future work to express the factors with biological parameters of an LIF neuron.

	\begin{figure}
		\centering
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/4-1.png}
			\caption{$\tau_{syn}$=1~ms}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/4-10.png}
			\caption{$\tau_{syn}$=10~ms}
		\end{subfigure}
		\caption{Noisy Softplus fits to the actual response firing rates of LIF neurons in concrete physical units.
			Recorded response firing rate of an LIF neuron driven by synaptic current with two synaptic time constants: (a) $\tau_{syn}$=10~ms and (b) $\tau_{syn}$=10~ms. Averaged firing rates of simulation trails are shown in bold lines, and the grey colour fills the range between the minimum to maximum of the firing rates. The thin lines are the scaled Noisy Softplus.}
		\label{Fig:nsptau1}
	\end{figure}

	\subsection{Parametric Activation Functions~(PAFs)}
	\begin{figure}[tbh!]
		\centering
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/neuron_o.pdf}
			\caption{Noisy Softplus}
		\DIFdelbeginFL %DIFDELCMD < \label{fig:noisy-softplus-neuron}
%DIFDELCMD < 	%%%
\DIFdelendFL \end{subfigure}~~~
		\begin{subfigure}[t]{0.42\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/neuron_PAF.pdf}
			\caption{Parametric Noisy Softplus}
		\DIFdelbeginFL %DIFDELCMD < \label{fig:parametric-noisy-softplus-neuron}
%DIFDELCMD < 	%%%
\DIFdelendFL \end{subfigure}
		\caption{The PAF links the firing activity of a spiking neuron to the numerical value of ANNs.}
		\label{Fig:tneuron}
	\end{figure}

	
	Neurons in ANNs take inputs from their previous layer, and feed the weighted sum of their input, $net_j = \sum_i w_{ij}x_i$, to \DIFdelbegin \DIFdel{their }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend activation function.
	The transformed signal then forms the output of an artificial neuron, which can be denoted as $y_j=f(net_j)$, see Figure~\ref{Fig:compare_as}\DIFdelbegin \DIFdel{a}\DIFdelend \DIFaddbegin \DIFadd{(a)}\DIFaddend .
	However, a spiking neuron modelled by Noisy Softplus takes the firing rate as \DIFdelbegin \DIFdel{its }\DIFdelend input/output \DIFdelbegin \DIFdel{, thus , }\DIFdelend \DIFaddbegin \DIFadd{thus }\DIFaddend Equation~\ref{equ:distr} can be written as:
	\begin{equation}
	net_j = \sum_i w_{ij}(\lambda_{i}\tau_{syn})~,
	~\sigma^2_j= \sum_i (\frac{1}{2} w_{ij}^2)(\lambda_{i}\tau_{syn})~, 
	% \textrm{~~and~~} x_i = \lambda_{i}\tau_{syn}~.
	\label{equ:mi_input}
	\end{equation}
	and \DIFdelbegin \DIFdel{$x_{i}$ }\DIFdelend \DIFaddbegin \DIFadd{$ x_i $ }\DIFaddend can be seen as $\lambda_{i}\tau_{syn}$\DIFdelbegin \DIFdel{(Fig.~\ref{fig:noisy-softplus-neuron}}\DIFdelend \DIFaddbegin \DIFadd{, see Figure~\ref{Fig:tneuron}(a}\DIFaddend ).

	\DIFdelbegin \DIFdel{We can build a structure around a PAF version of Noisy Softplus which will behave exactly as a common ANN unit (Figure~\ref{fig:parametric-noisy-softplus-neuron})where }\DIFdelend \DIFaddbegin \DIFadd{If we move the left end process of $\times \tau_{syn}$ to the right end after $\lambda_j$, Figure~\ref{Fig:tneuron}(b) forms the same neuron model and structure as a typical neuron in ANNs, See Figure~\ref{Fig:compare_as}(a), that }\DIFaddend neurons take $x$ as input and \DIFdelbegin \DIFdel{output }\DIFdelend \DIFaddbegin \DIFadd{outputs }\DIFaddend $y$.
	The \DIFdelbegin \DIFdel{PAF version of }\DIFdelend \DIFaddbegin \DIFadd{only difference lies in }\DIFaddend the activation function \DIFdelbegin \DIFdel{(Eq.~\ref{equ:PAF}) will be }\DIFdelend \DIFaddbegin \DIFadd{where the artificial spiking neuron takes PAF, which is a simple }\DIFaddend linearly-scaled \DIFdelbegin \DIFdel{by the conversion }\DIFdelend \DIFaddbegin \DIFadd{activation function with the scaling }\DIFaddend parameter $S$ and the synaptic time constant\DIFaddbegin \DIFadd{, }\DIFaddend $\tau_{syn}$:
	\begin{equation}
	y = PAF(x) = f(x) \times S \times \tau_{syn}~,
	\label{equ:PAF}
	\end{equation}
	and its derivative function \DIFdelbegin \DIFdel{, }\DIFdelend which is used \DIFdelbegin \DIFdel{for back propagation , }\DIFdelend \DIFaddbegin \DIFadd{with back propagation }\DIFaddend is:
	\begin{equation}
	\frac{\partial y}{\partial x} = f'(x) \times S \times \tau_{syn}~~.
	\end{equation}

	Excitingly, PAF not only allows Noisy Softplus to model spiking LIF neurons on ANNs, but also can be generalised to other activation functions.
	Note that the calculation of noise level is not necessary for other activation functions, for example, it can be set to a constant for Softplus or 0 for ReLU.
	%\begin{figure}[bh!]
	%	\centering
	%	\includegraphics[width=0.8\textwidth]{pics_iconip/neuron_o.pdf}
	%	\caption{Artificial spiking neuron takes scaled firing rates as input, then transforms weighted sum in some activation unit to its output which can be scaled-up to the firing rate of an output spike train.}
	%	\label{Fig:sneuron}
	%\end{figure}
	%
	%Noisy Softplus transforms the noisy current with parameters of $(net_j, \sigma_j)$ to the equivalent ANN output $y_j$ , where it can be scaled up by the factor $S$ to the firing rate of SNNs.
	%Note that the calculation of noise level is not necessary for activation functions other than Noisy Softplus, for example, it can be set to a constant for Softplus or 0 for ReLU.
	%We name the neuron model `artificial spiking neurons' which takes firing rates of spike trains as input and output. 
	%The entire artificial spiking neuron model is then generalised to any ReLU/Softplus-like activation functions, See Figure~\ref{Fig:sneuron}.
	%
	%
	%
	%%	Figure~\ref{Fig:sneuron} shows an complete transformation process of a spiking neuron, which mimics the biological neurons taking and generating spike trains.
	%Referred to Figure~\ref{Fig:sneuron}, if we move the left end process of $\times \tau_{syn}$ to the right end after $\lambda_j$, Figure~\ref{Fig:sneuron} forms the same neuron model and structure as multilayer perceptron: neurons take $x$ as input and outputs $y$, and this conversion is illustrated in Figure~\ref{Fig:tneuron}.
	%The process within such an artificial neuron is divided into weighted summation and activation, which also applies to SNN modelling by combining the scaling factor $S$ and the synaptic time constant $\tau_{syn}$ to activation functions.
	%Thus the combined activation function for modelling SNNs should be:
	%\begin{equation}
	%y = f(x) \times S \times \tau_{syn}~~,
	%\label{equ:full_act}
	%\end{equation}
	%and its derivative function which is used when back propagates is:
	%\begin{equation}
	%\frac{\partial y}{\partial x} = f'(x) \times S \times \tau_{syn}~~.
	%\end{equation}
	%
	%\begin{figure}[tbh!]
	%	\centering
	%	\includegraphics[width=0.8\textwidth]{pics_iconip/neuron_t.pdf}
	%	\caption{Transforming artificial spiking neurons to artificial neurons for SNN modelling. The combined activation links the firing activity of a spiking neuron to the numerical value of ANNs.}
	%	\label{Fig:tneuron}
	%\end{figure}

	\subsection{Generalised SNN Training}
	\label{subsec:ns_train}
	Most excitingly, SNN training \DIFaddbegin \DIFadd{then }\DIFaddend can be simplified to: calibrate the parameters \DIFdelbegin \DIFdel{$k$ and $S$ }\DIFdelend \DIFaddbegin \DIFadd{of $(k, S)$ }\DIFaddend for Noisy Softplus which models response firing rates of LIF neurons, and use $S$ and $\tau_{syn}$ for PAF-ReLU as the activation function when training.
	\DIFdelbegin \DIFdel{Parameters $k$ and $S$ depend only on the characterization }\DIFdelend \DIFaddbegin \DIFadd{Note that, $(k, S)$ are dependent on the biological configurations }\DIFaddend of an LIF neuron, \DIFdelbegin \DIFdel{thus, once they are tuned for a certain LIF configuration, they can be used for various activation functions and different network configurations.
	}\DIFdelend \DIFaddbegin \DIFadd{but independent from the core activation functions used in PAF.
	%DIF > Thus, once the parameters are tuned for a certain LIF configuration, they can be used for trainings with various activation functions.
	}\DIFaddend 

	\DIFdelbegin \DIFdel{A small modification is required to use standard activation functions for ANNs , the change is a linear scaling which can be embedded in usual ANN parameters such as the learning rate.
	%DIF < which may be of low computational complexity and their corresponding derivative functions can be directly used for back propagation.
In particular using PAF-ReLU improves training performance of SNNs, since its, to the best of our knowledge}\DIFdelend %DIF > todo
	\DIFaddbegin \DIFadd{This generalised SNN training allows the use of widely-used activation functions in ANNs which are of low complexity and their corresponding derivative functions can be directly used for back propagation.
	Especially, ReLU}\DIFaddend , the simplest and most effective activation function \DIFaddbegin \DIFadd{may improve the training performance of SNNs}\DIFaddend .
	Ideally, the method can be applied for any \DIFdelbegin \DIFdel{feed-forward }\DIFdelend \DIFaddbegin \DIFadd{feedforward }\DIFaddend network using ReLU-like activation functions, including deep architectures.
	\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Most significantly, the ANN-trained weights are \DIFdelbegin \DIFdel{ready-to-use (i.e. no extra conversion is required) and can be directly transferred to SNNs . An added benefit of PAF is that }\DIFdelend \DIFaddbegin \DIFadd{ready for use to transfer to SNNs without any conversion, and }\DIFaddend the output firing rate of a spiking \DIFdelbegin \DIFdel{neurons }\DIFdelend \DIFaddbegin \DIFadd{neuron }\DIFaddend can be obtained in the ANN simulation \DIFdelbegin \DIFdel{. These rates can then used }\DIFdelend \DIFaddbegin \DIFadd{thus }\DIFaddend to estimate the power \DIFdelbegin \DIFdel{usage or communication requirements for the networks }\DIFdelend \DIFaddbegin \DIFadd{use }\DIFaddend in Neuromorphic systems (hardware SNN simulators\DIFdelbegin \DIFdel{; e.g. SpiNNaker, TrueNorth, NeuroGrid, BrainScales~\mbox{%DIFAUXCMD
\cite{liu2016bench}
}%DIFAUXCMD
}\DIFdelend ).

	
	
	\subsection{Fine Tuning}
	\DIFaddbegin \DIFadd{We can train the network with any PAF as stated above, and then fine-tune it with PAF-Noisy-Softplus to take account of both the accuracy and practical network activities of SNNs.
	}\DIFaddend The labels of data are always converted to binary values for ANN training\DIFdelbegin \DIFdel{,this }\DIFdelend \DIFaddbegin \DIFadd{.
	This }\DIFaddend enlarges the disparities between the correct recognition label and the rest to train the network for better classification capability.
	\DIFdelbegin \DIFdel{Consequently, we can train the network with any activation function and then fine-tune it with Noisy Softplus to take account of both the accuracy and practical network activities of SNNs.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{For the fine tuning procedure we present the full training data once, though this time }\DIFdelend \DIFaddbegin \DIFadd{However, }\DIFaddend we add a small number, for example 0.01, to all the binary values of the data labels.
	Doing so helps the training to relax the strict objective function to predict exact labels with binary values\DIFdelbegin \DIFdel{and it models noise activity in the last layer}\DIFdelend \DIFaddbegin \DIFadd{.
	Instead, it allows a small offset to the objective}\DIFaddend .
	An alternative method is to use \DIFdelbegin \DIFdel{the }\DIFdelend Softmax function at the top layer, which aims to map real vectors to the range $(0,1)$ \DIFdelbegin \DIFdel{which }\DIFdelend \DIFaddbegin \DIFadd{that }\DIFaddend add up to 1. 
	However, without a limit on the input of Softmax, it will be easy to reach or even exceed the highest firing rate of a spiking neuron.
	The result of fine tuning on a \DIFdelbegin \DIFdel{ConvNet }\DIFdelend \DIFaddbegin \DIFadd{Convnet }\DIFaddend will be demonstrated in subsection~\ref{subsec:result_compare}.

	There are two aspects to the fine tuning which make \DIFdelbegin \DIFdel{ANNs behave }\DIFdelend \DIFaddbegin \DIFadd{the ANN }\DIFaddend closer to SNNs:
	\DIFdelbegin \DIFdel{First}\DIFdelend \DIFaddbegin \DIFadd{Firstly}\DIFaddend , using the Noisy Softplus activation \DIFdelbegin \DIFdel{function }\DIFdelend \DIFaddbegin \DIFadd{functions }\DIFaddend causes every single neuron to run \DIFdelbegin \DIFdel{at }\DIFdelend \DIFaddbegin \DIFadd{as }\DIFaddend a similar noise level as in SNNs, thus the weights trained by other activation functions will be tuned to fit closer to SNNs.
	\DIFdelbegin \DIFdel{Second}\DIFdelend \DIFaddbegin \DIFadd{Secondly}\DIFaddend , the output firing rate of any LIF neuron is greater than zero as long as noise exists in their synaptic input.
	Thus adding up a small offset on the labels directs the model to approximate practical SNNs. 

	\section{Results}
	\label{sec:result}
	\DIFaddbegin 

	\subsection{\DIFadd{Experiment Set-up}}
	\DIFaddend A convolutional network model was trained on MNIST,
	%	~\cite{lecun1998gradient}
	a popular database in neuromorphic vision, using the \DIFdelbegin \DIFdel{ANN-trained SNN }\DIFdelend \DIFaddbegin \DIFadd{generalised SNN training }\DIFaddend method stated above.
	The architecture contains $28\times28$ input units, followed by two convolution-pooling layers and 10 output neurons fully connected to the last pooling layer to represent the classified digit.

%DIF > 	The training only employed Noisy Softplus units, which means all the convolution, average sampling, and the fully-connected neurons use Noisy Softplus function.
	\DIFaddbegin \DIFadd{The training employed PAFs and was compared with three core activation functions: ReLU, Softplus and Noisy Softplus.
	}\DIFaddend The \DIFdelbegin \DIFdel{training only employed Noisy Softplus units for all the convolution, average sampling, and fully-connected neurons.
	%DIF <  use Noisy Softplus function with no bias.
The parameters of the activation function were calibrated as , }\DIFdelend \DIFaddbegin \DIFadd{parameter p was estimated as $p = S \times \tau_{syn} = 1.005$ by calibrating the Noisy Softplus with LIF response firing rate: }\DIFaddend $(k=0.30, S=201)$ \DIFdelbegin \DIFdel{,  for LIF neurons}\DIFdelend \DIFaddbegin \DIFadd{given $\tau_{syn}=5 ms$}\DIFaddend .
	The pixel intensities of the input images were normalised to 100~\textit{Hz} to \DIFdelbegin \DIFdel{represent }\DIFdelend \DIFaddbegin \DIFadd{present }\DIFaddend the firing rates of the input spikes.
	The weights were updated using a decaying learning rate, 50 images per batch and 20 epochs.
	\DIFdelbegin \DIFdel{These }\DIFdelend \DIFaddbegin \DIFadd{The ANN-trained }\DIFaddend weights were then directly \DIFdelbegin \DIFdel{applied }\DIFdelend \DIFaddbegin \DIFadd{transferred }\DIFaddend to the corresponding \DIFdelbegin \DIFdel{convolutional SNN  }\DIFdelend \DIFaddbegin \DIFadd{spiking ConvNet without any conversion }\DIFaddend for recognition tasks.

	
	\subsection{Neural Activity}
	To validate how well the Noisy Softplus activation fits to the response firing rate of LIF neurons in \DIFdelbegin \DIFdel{a real application}\DIFdelend \DIFaddbegin \DIFadd{SNNs}\DIFaddend , we simulated the \DIFdelbegin \DIFdel{model on NEST}\DIFdelend \DIFaddbegin \DIFadd{trained spiking ConvNet on NEST~\mbox{%DIFAUXCMD
\cite{gewaltig2007nest}
}%DIFAUXCMD
}\DIFaddend using the Poisson MNIST dataset~\cite{liu2016bench}.
	%DIF <  and the neurons of a convolutional map were observed.

	A small test consisted of ten MNIST digits presented as Poisson spike trains \DIFaddbegin \DIFadd{for 1~s each.
	A PAF-Noisy-Softplus trained $5\times5$ kernel was }\DIFaddend convolved with these input digits, \DIFaddbegin \DIFadd{and }\DIFaddend the output firing \DIFdelbegin \DIFdel{rate was recorded during an SNN simulation on NEST, }\DIFdelend \DIFaddbegin \DIFadd{rates were recorded }\DIFaddend and compared to the \DIFdelbegin \DIFdel{modelled activations of Equation~\ref{equ:PAF} in ANNs}\DIFdelend \DIFaddbegin \DIFadd{scaled activation functions: $S \times f(x)$}\DIFaddend .

	%The input $x$ of the network was calculated as Equation~\ref{equ:mi_input}: $x_i=\lambda_i\tau_{syn}$, and so as the weighted sum of the synaptic current (see Equation~\ref{equ:mi_input}), $net_j$ and its variance (see Equation~\ref{equ:si_input}), $\sigma^2_j$.
	%With three combined activation functions as Equation~\ref{equ:full_act}:
	%\begin{equation}
	%\begin{aligned}
	%&\textrm{(1) Noisy Softplus:~~}  y_j=k \sigma_j \log [1 + \exp(\frac{net_j}{k \sigma_j})] \times S \times \tau_{syn}~~,  \\
	%&\textrm{(2) ReLU:~~ } y_j=max(0, net_j) \times S \times \tau_{syn}~~, \\
	%&\textrm{(3) Softplus:~~ } y_j=k \sigma \log [1 + \exp(\frac{net_j}{k \sigma})] \times S \times \tau_{syn}~~, ~~~\sigma=0.45,  
	%\end{aligned}
	%\end{equation}	
\DIFdelbegin \DIFdel{With PAF versions of ReLU, Softplus and Noisy Softplus, we compare the output to the recorded SNN simulations.
}\DIFdelend %DIF > 	With three PAFs of ReLU, Softplus and Noisy Softplus, we compare the output to the recorded SNN simulations.
	%ReLU assumes a non-noise current, and Softplus takes a static noise level thus $\sigma_j$ is not used for either of them, meanwhile Noisy Softplus adapts to noise automatically with $\sigma_j$.
	The experiment took the sequence of 10 digits to the same kernel and the estimated spike counts using Noisy Softplus fitted the real recorded firing rate much more accurately than ReLU and Softplus, see Figure~\ref{fig:af_compare}.
	%	Figure~\ref{fig:af_stast} illustrated the statistics of error between the estimation and the recorded firing rate, $err = y_j - \lambda_j$ which formed normal distributions where Noisy Softplus hold the weakest mean (low in abstract) and lowest standard deviation.
	The Euclidean distance, $\sqrt{\sum_{j}(y_j/\tau_{syn} - \lambda_j)^2}$, between the spike counts and the predicted firing rates by Noisy Softplus, ReLU and Softplus was 184.57, 361.64 and 1102.76 respectively.
	We manually selected a static noise level of 0.45 for Softplus, whose estimated firing rates located roughly on the top slope of the real response activity.
	This resulted in a longer Euclidean distance than using ReLU, since most of the input noisy currents were of relatively low noise level in this experiment.
	Hence, the firing rate driven by the lower noise level is closer to ReLU curve than Softplus.

	\begin{figure}[hb!]
		\centering
		\begin{subfigure}[hb]{0.32\textwidth}
			\DIFdelbeginFL %DIFDELCMD < \includegraphics[width=\textwidth]{pics_iconip/6-5-1.png}
%DIFDELCMD < 		%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width=\textwidth]{pics_iconip/6-5-3.png}
			\DIFaddendFL \caption{Softplus}
		\end{subfigure}
		\begin{subfigure}[hb]{0.32\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/6-5-2.png}
			\caption{ReLU}
		\end{subfigure}
		\begin{subfigure}[hb]{0.32\textwidth}
			\DIFdelbeginFL %DIFDELCMD < \includegraphics[width=\textwidth]{pics_iconip/6-5-3.png}
%DIFDELCMD < 		%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width=\textwidth]{pics_iconip/6-5-1.png}
			\DIFaddendFL \caption{Noisy Softplus}
		\end{subfigure}
		\caption{\DIFdelbeginFL \DIFdelFL{Noisy Softplus fits to the neural response firing rate in an SNN simulation.
			}\DIFdelendFL 
			The recorded firing rate of the \DIFaddbeginFL \DIFaddFL{convolution of the }\DIFaddendFL same kernel \DIFdelbeginFL \DIFdelFL{convolved }\DIFdelendFL with 10 images in SNN simulation, compared to the \DIFaddbeginFL \DIFaddFL{firing rate }\DIFaddendFL prediction \DIFdelbeginFL \DIFdelFL{of activations of }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{by $S \times f(x)$.
			Noisy }\DIFaddendFL Softplus \DIFdelbeginFL \DIFdelFL{, }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{(a) fits to the neural response firing rate of LIF neurons more closely than }\DIFaddendFL ReLU \DIFdelbeginFL \DIFdelFL{, }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{(b) }\DIFaddendFL and \DIFdelbeginFL \DIFdelFL{Noisy }\DIFdelendFL Softplus \DIFaddbeginFL \DIFaddFL{(c)}\DIFaddendFL .}
		\label{fig:af_compare}
	\end{figure}		

\DIFdelbegin \DIFdel{The SNN successfully classified the digits where the correct label neuron fired the most.
We trained the network with binary labels on the output layer, thus the expected }\DIFdelend %DIF > 	The SNN successfully classified the digits where the correct label neuron fired the most.
%DIF > 	We trained the network with binary labels on the output layer, thus the expected firing rate of correct classification was $1/\tau_{syn}=200$~\textit{Hz} according to Equation~\ref{Fig:tneuron}.
%DIF > 	The firing rates of the recognition test fell to the valid range around 0 to 200~\textit{Hz}.
	\DIFaddbegin \DIFadd{This shows another advantage of the Noisy Softplus activation function that we can estimate the }\DIFaddend firing rate of \DIFdelbegin \DIFdel{correct classification was $1/\tau_{syn}=200$~}\textit{\DIFdel{Hz}} %DIFAUXCMD
\DIFdel{according to Equation~\ref{Fig:tneuron}.
The firing rates of the recognition test fell to the valid range 0 to 200~}\textit{\DIFdel{Hz}}%DIFAUXCMD
\DIFdel{.
	This shows another advantage of the proposed ANN-trained method}\DIFdelend \DIFaddbegin \DIFadd{an SNN by $S \times f_{NS}(x)$ running its equivalent ANN, instead of simulating SNNs.
	Moreover}\DIFaddend , we can constrain the expected firing rate of the top layer, thus preventing the SNN from exceeding its maximum firing rate, for example 1000~\textit{Hz} when the time resolution of the SNN simulation is set to 1~ms.

	
	\subsection{\DIFdelbegin \DIFdel{Recognition }\DIFdelend \DIFaddbegin \DIFadd{Learning }\DIFaddend Performance}
	\DIFdelbegin %DIFDELCMD < \label{subsec:result_compare}
%DIFDELCMD < %%%
\DIFdelend %Here we focus on the recognition performance of the proposed ANN-trained SNN method.
	Before looking into the recognition results, it is significant to see the learning capability of the proposed activation function, Noisy Softplus.
	We compared the training using ReLU, Softplus, and Noisy Softplus by their loss during training averaged over 3 trials, see Figure~\ref{Fig:loss_ns}.
	ReLU learned fastest with the lowest loss, thanks to its steepest derivative.
	In comparison, Softplus accumulated spontaneous firing rates \DIFdelbegin \DIFdel{layer-by-layer }\DIFdelend \DIFaddbegin \DIFadd{layer by layer }\DIFaddend and its derivative may experience vanishing gradients during back propagation, which result in a more difficult training.
	The Noisy Softplus performance \DIFdelbegin \DIFdel{lies }\DIFdelend \DIFaddbegin \DIFadd{lay }\DIFaddend between these two in terms of loss and learning speed.
	However, the loss stabilised fastest, which means a possibly shorter training time.
	%\begin{figure}[tbp!]
	%	\centering
	%	\includegraphics[width=0.7\textwidth]{pics_iconip/8.png}
	%	\caption{Comparisons of Loss during training using Noisy Softplus, ReLU and Softplus activation functions. Bold lines show the average of three training trials, and the grey colour illustrates the range between the minimum and the maximum values of the trials.  }
	%	\label{Fig:loss_ns}
	%\end{figure}
	%	The trained networks were scaled to SNNs and compared on recognition rates, 93.34\%, 96.43\% and 97.03\% with a conversion loss of 4.76\%, 0.91\% and 0.74\%.
	\DIFdelbegin %DIFDELCMD < \begin{figure}[tbh!]
%DIFDELCMD <   \centering
%DIFDELCMD <   \begin{subfigure}[t]{0.48\textwidth}
%DIFDELCMD <     \includegraphics[width=\textwidth]{pics_iconip/8.png}
%DIFDELCMD <     %%%
\DIFdelend \DIFaddbegin 

	\begin{figure}
		\begin{minipage}[t]{0.48\linewidth}
			\raggedleft
			\includegraphics[width=2.6in]{pics_iconip/8.png}
			\DIFaddendFL \caption{Comparisons of Loss during training using \DIFaddbeginFL \DIFaddFL{PAF version of }\DIFaddendFL Noisy Softplus, ReLU and Softplus\DIFdelbeginFL \DIFdelFL{activation functions}\DIFdelendFL . Bold lines show the average of three training trials, and the grey colour illustrates the range between the minimum and the maximum values of the trials. }
			\label{Fig:loss_ns}
		\DIFdelbeginFL %DIFDELCMD < \end{subfigure}%%%
\DIFdelFL{~~~
  }%DIFDELCMD < \begin{subfigure}[t]{0.48\textwidth}
%DIFDELCMD <     \includegraphics[width=\textwidth]{pics_iconip/9-2.pdf}
%DIFDELCMD <     %%%
\DIFdelendFL \DIFaddbeginFL \end{minipage}%DIF > 
		\DIFaddFL{\hspace{0.04\linewidth}
		}\begin{minipage}[t]{0.48\linewidth}
			\raggedright
			\includegraphics[width=2.6in]{pics_iconip/9-2.pdf}
			\DIFaddendFL \caption{Classification accuracy\DIFdelbeginFL \DIFdelFL{of Noisy Softplus, ReLU, Softplus on original DNNs~}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{.
			The trained weights were tested using the same activation function as training }\DIFaddendFL (DNN\_Orig), \DIFaddbeginFL \DIFaddFL{then tested using }\DIFaddendFL Noisy Softplus \DIFdelbeginFL \DIFdelFL{tested DNN~}\DIFdelendFL (DNN\_NS) \DIFaddbeginFL \DIFaddFL{and transferred to SNN test (SNN}\\\DIFaddFL{SUBSCRIPTNB}{\DIFaddFL{O}}\DIFaddFL{rig)}\DIFaddendFL , \DIFaddbeginFL \DIFaddFL{and finally }\DIFaddendFL fine-tuned \DIFdelbeginFL \DIFdelFL{DNN }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{to be }\DIFaddendFL tested \DIFdelbeginFL \DIFdelFL{on }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{using }\DIFaddendFL Noisy Softplus \DIFdelbeginFL \DIFdelFL{~}\DIFdelendFL (DNN+FT\_NS) \DIFdelbeginFL \DIFdelFL{, }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{and on }\DIFaddendFL SNN \DIFdelbeginFL \DIFdelFL{with original weights~}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{simulation }\DIFaddendFL (SNN\\SUBSCRIPTNB{\DIFdelbeginFL \DIFdelFL{O}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{F}\DIFaddendFL }\DIFdelbeginFL \DIFdelFL{rig) and SNN with fine-tuned weights~(SNN+FT}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{T}\DIFaddendFL ).  }
			\label{Fig:result_bar}
		\DIFdelbeginFL %DIFDELCMD < \end{subfigure}
%DIFDELCMD <   %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Comparisons on training and testing of the ConvNet.}}
  %DIFAUXCMD
%DIFDELCMD < \label{Fig:performance}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \end{minipage}
	\DIFaddendFL \end{figure}
	%DIF < \begin{figure}
%DIF < 	\begin{minipage}[t]{0.48\linewidth}
%DIF < 		\raggedleft
%DIF < 		\includegraphics[width=2.6in]{pics_iconip/8.png}
%DIF < 		\caption{Comparisons of Loss during training using Noisy Softplus, ReLU and Softplus activation functions. Bold lines show the average of three training trials, and the grey colour illustrates the range between the minimum and the maximum values of the trials. }
%DIF < 		\label{Fig:loss_ns}
%DIF < 	\end{minipage}%
%DIF <     \hspace{0.01\linewidth}
%DIF < 	\begin{minipage}[t]{0.48\linewidth}
%DIF < 		\raggedright
%DIF < 		\includegraphics[width=2.6in]{pics_iconip/9-2.pdf}
%DIF < 		\caption{Classification accuracy compared among trained weights of Noisy Softplus, ReLU, Softplus on DNN, SNN and fine-tuned SNN.}
%DIF < 		\label{Fig:result_bar}
%DIF < 	\end{minipage}
%DIF < \end{figure}

	\DIFaddbegin \subsection{\DIFadd{Recognition Performance}}
	\label{subsec:result_compare}
	\DIFaddend The recognition test took the whole testing dataset of MNIST which contains $10,000$ images.
	At first, all trained models were tested on the same artificial neurons as used for training in ANNs, and these experiments were called the `DNN' test since the network had a deep structure (6 layers).
	Subsequently, the trained weights were directly applied to the SNN without any transformation, and these `SNN' experiments tested their recognition performance on the NEST simulator.
	The LIF neurons had the same parameters as in training.
	The input images were converted to Poisson spike trains and presented for 1~s each.
	The output neuron which fired the most indicated the classification of an input image.
	Moreover, a `\DIFdelbegin \DIFdel{Fine }\DIFdelend \DIFaddbegin \DIFadd{fine }\DIFaddend tuning' test took the trained model for fine tuning, and the tuned weights were tested on the same SNN \DIFdelbegin \DIFdel{environt}\DIFdelend \DIFaddbegin \DIFadd{environment}\DIFaddend .
	The tuning only ran for one epoch, 5\% of the cost of the ANN training (20~epochs), using Noisy Softplus neurons with labels shifted by $+0.01$.

	The classification errors for the tests are investigated by comparing the average classification accuracy, shown in Figure~\ref{Fig:result_bar}.
	From DNN to SNN, the classification accuracy declines by 0.80\%, 0.79\% and 3.12\% on average for Noisy \DIFdelbegin \DIFdel{Softplus}\DIFdelend \DIFaddbegin \DIFadd{softplus}\DIFaddend , ReLU and Softplus
	\DIFdelbegin \DIFdel{, respectively.
}\DIFdelend The accuracy loss was caused by the mismatch between the activations and the practical response firing rates, see example in Figure~\ref{fig:af_compare}, and the strict binary labels for Noisy Softplus and Softplus activations.
	Fortunately, the problem is alleviated by fine tuning which increased the classification accuracy by 0.38\%, 0.19\% and 2.06\%, and resulted in the total loss of 0.43\%, 0.61\%, and 1.06\% respectively.
	The improvement of ReLU is not as great as the others, because there is no problem of strict labels during training.
	%	Thus fine tuning mainly corrects the mismatch between ReLU and the firing rates in SNNs, and constraints the output firing rates of the network.
	Softplus benefits the most from fine tuning, since not only the huge mismatch of response firing rate is greatly corrected, but also the offset on the labels helps the network to fit SNNs. 

	%\begin{figure}[hbt!]
	%	\centering
	%	\includegraphics[width=0.7\textwidth]{pics_iconip/9-2.pdf}
	%	\caption{Classification accuracy compared among trained weights of Noisy Softplus, ReLU, Softplus on DNN, SNN and fine-tuned SNN.}
	%	\label{Fig:result_bar1}
	%\end{figure}

	%\begin{table}[tbh] 
	%	\caption{Comparisons of classification accuracy (in \%) of ANN-trained convolutional neural models on original DNN, NEST simulated SNN, and SNN with fine-tuned (FT) model.}
	%	\begin{center}
	%		\bgroup
	%		\def\arraystretch{1.5}
	%		\begin{tabular} {r |c  c c |c c c |c c c}
	%			%First line
	%			\hline
	%			Trial No.
	%			&\multicolumn{3}{c|}{1} 
	%			&\multicolumn{3}{c|}{2}
	%			&\multicolumn{3}{c}{3}\\
	%			\hline
	%			Model
	%			& DNN & SNN &FT
	%			& DNN & SNN &FT
	%			& DNN & SNN &FT\\
	%			\hline
	%			\textbf{Noisy Sofplus}
	%			& 1.91 & 2.76 &2.45
	%			& 1.79 & 2.56 &2.19
	%			& 1.76 & 2.55 &2.10\\
	%			%				& 98.09 & 97.24 &97.55
	%			%				& 98.21 & 97.44 &97.81
	%			%				& 98.24 & 97.45 &97.90\\
	%			%				\hline
	%			\textbf{ReLU}
	%			& 1.36 & 2.03 &1.88
	%			& 1.46 & 2.28 &2.00
	%			& 1.36 & 2.25 &2.12\\
	%			%				& 98.64 & 97.97 &98.12
	%			%				& 98.54 & 97.72 &98.00
	%			%				& 98.64 & 97.75 &97.88\\
	%			%				\hline
	%			\textbf{Sofplus}
	%			& 2.30 & 5.66 &3.91
	%			& 2.75 & 5.22 &3.55
	%			& 2.42 & 6.62 &3.87\\
	%			%				& 97.70 & 94.34 &96.09
	%			%				& 97.25 & 94.78 &96.45
	%			%				& 97.58 & 93.38 &96.13\\
	%			\hline
	%		\end{tabular}
	%		\egroup
	%		\label{tbl:ns_result}
	%	\end{center}
	%\end{table}

	
	
	
	%	The best classification accuracy achieved by SNNs was trained with ReLU and fine-tuned by Noisy Softplus.
	The most efficient training in terms of both classification accuracy and \DIFdelbegin \DIFdel{algorithmic }\DIFdelend \DIFaddbegin \DIFadd{algorithm }\DIFaddend complexity, takes ReLU for ANN training and Noisy Softplus for fine tuning.
	Softplus \DIFdelbegin \DIFdel{exhibits the worst }\DIFdelend \DIFaddbegin \DIFadd{does not exhibit better }\DIFaddend classification capability and, more importantly, the manually selected static noise level hugely influences the mismatch between the predicted firing rates and the real data.
	Although Noisy Softplus shows the least classification drop from ANNs to SNNs, the training performance is still worse than ReLU.
	%	Improved back propagation or other learning algorithms using noise level will be listed in the future work. 

	The best classification accuracy achieved by an SNN was 98.85\%, a 0.20\% drop from the ANN test (99.05\%), which was trained with ReLU and fine-tuned using Noisy Softplus.
	It is useful to compare with existing SNN training methods in Table~\ref{tbl:compare} where we order them on the computation complexity (descending).
	The generalised training method \DIFdelbegin \DIFdel{presented in this paper has a simple }\DIFdelend \DIFaddbegin \DIFadd{this paper proposed wins over the simplest }\DIFaddend activation function, \DIFdelbegin \DIFdel{it requires no conversions }\DIFdelend \DIFaddbegin \DIFadd{and no conversions are requited }\DIFaddend of trained weights \DIFdelbegin \DIFdel{, which }\DIFdelend \DIFaddbegin \DIFadd{to be transferred to SNNs, they }\DIFaddend are well fitted to biologically-plausible LIF neurons, and \DIFdelbegin \DIFdel{has a single }\emph{\DIFdel{optional}} %DIFAUXCMD
\DIFdel{extra trick }\DIFdelend \DIFaddbegin \DIFadd{only an optional additive processing }\DIFaddend of fine tuning.
	\DIFdelbegin \DIFdel{The combination of these features compose a method with exceptional performance and ease of use for training SNNs.
  }\DIFdelend \DIFaddbegin 

	
	
  \begin{table*}[thb!]
  	\caption{SNN training methods comparison.}
  	\begin{center}
  		\bgroup
  		\def\arraystretch{1.1}
  		\begin{tabular}{l c c c c c c}
  			\begin{mycell}{1cm} Method \end{mycell} & 
  			%  					\begin{mycell}{1.8cm} Computation\\Complexity \end{mycell} & 
  			\begin{mycell}{1.8cm}Activation\\Function\end{mycell} &
  			\begin{mycell}{1.8cm} Biologically-\\plausibility \end{mycell} &  
  			\begin{mycell}{1.8cm} Additional\\Processing \end{mycell} &
  			\begin{mycell}{1.8cm} Conversion \end{mycell} & 
  			\begin{mycell}{1.8cm} Classification\\Accuracy(\%) \end{mycell} 
  			\\
  			\hline
  			\begin{mycell}{1cm} \cite{Jug_etal_2012} \end{mycell} & 
  			%  					\begin{mycell}{1.8cm} 1st \end{mycell} & 
  			\begin{mycell}{1.8cm}Siegert \end{mycell} &
  			\begin{mycell}{1.8cm} \textbf{Yes} \end{mycell} &  
  			\begin{mycell}{1.8cm} \textbf{No} \end{mycell} & 
  			\begin{mycell}{1.8cm} \textbf{No} \end{mycell} & 
  			\begin{mycell}{1.8cm} 94.94~\cite{Stromatias2015scalable} \end{mycell} 
  			\\
  			\begin{mycell}{1cm} \cite{hunsberger2015spiking} \end{mycell} & 
  			%  					\begin{mycell}{1.8cm} 2nd \end{mycell} & 
  			\begin{mycell}{1.8cm} Soft LIF \end{mycell} &
  			\begin{mycell}{1.8cm} \textbf{Yes} \end{mycell} &  
  			\begin{mycell}{2.2cm} Noisy inputs\\ \&Activations \end{mycell} & 
  			\begin{mycell}{1.8cm} \textbf{No} \end{mycell} & 
  			\begin{mycell}{1.8cm} 98.37 \end{mycell}
  			\\
  			\begin{mycell}{1cm} \cite{diehl2015fast} \end{mycell} & 
  			%  					\begin{mycell}{1.8cm} 3rd \end{mycell} & 
  			\begin{mycell}{1.8cm} \textbf{ReLU} \end{mycell} &
  			\begin{mycell}{1.8cm} No \end{mycell} &  
  			\begin{mycell}{1.8cm} Dropout  \end{mycell} & %\&\\Conversion
  			\begin{mycell}{1.8cm} Yes \end{mycell} &  
  			\begin{mycell}{1.8cm} \textbf{99.1\%} \end{mycell} 
  			\\
  			\begin{mycell}{1cm} This\\Paper \end{mycell} & 
  			%  					\begin{mycell}{1.8cm} 3rd \end{mycell} & 
  			\begin{mycell}{1.8cm} \textbf{PAF}\\($p\times$ReLU)\end{mycell} &
  			\begin{mycell}{1.8cm} \textbf{Yes} \end{mycell} &  
  			\begin{mycell}{2.2cm} \textbf{No} \\or fine tune  \end{mycell} & 
  			\begin{mycell}{1.8cm} \textbf{No} \end{mycell} & 
  			\begin{mycell}{2.4cm} 98.70\\ 98.85(fine tune) \end{mycell}  
  			% contents!
  		\end{tabular}
  		\egroup
  	\end{center}
  	\label{tbl:compare}
  \end{table*}

	%DIF < \begin{table}[thb]
%DIF < 	\centering
%DIF < 	\caption{\label{tbl:compare}SNN training methods comparisons.}
%DIF < 	\bgroup
%DIF < 	\def\arraystretch{1.4}
%DIF < 	\begin{tabular}{c c c c c c c}
%DIF < 		%\hline
%DIF < 		Complexity & Activation\\function & conversion & biologically-plausible  & extra tricks \\
%DIF < 		\hline
%DIF < 	 	1~\cite{Jug_etal_2012} & Siegert & No & Yes & No \\
%DIF < 	 	2~\cite{hunsberger2015spiking} & Soft LIF & No & Yes & Noisy input \& AF\\
%DIF < 	 	3~\cite{diehl2015fast} & ReLU & Normalisation & No & Dropout \\
%DIF < 	 	4 This Paper & PAF-ReLU & No & Yes & No 
%DIF < 	\end{tabular}
%DIF < 	\egroup
%DIF < \end{table}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend %The network structure was the same with the state-of-the-art model which reported the best classification accuracy of 99.1\%~\cite{diehl2015fast} in ANN-trained SNNs: 12c5-2s-64c5-2s-10fc.
	%Their nearly loss-less conversion from ANNs to SNNs was achieved by using IF neurons, while our network performs the best among SNNs consisted of LIF neurons to our knowledge.

	\begin{figure}[htb!]
		\centering
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/8-2.png}
			\caption{Before fine tuning}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\textwidth}
			\includegraphics[width=\textwidth]{pics_iconip/8-3.png}
			\caption{After fine tuning.}
		\end{subfigure}

		\caption{The classification accuracy of 3 trials (averaged in bold lines, grey shading shows the range between minimum to maximum) over short response times, with (a) trained weights before fine tuning, and (b) after fine tuning.}
		\label{fig:ca_time}	
	\end{figure}

	As it is a major concern in neuromorphic vision, the recognition performance over short response times is also estimated \DIFdelbegin \DIFdel{~(}\DIFdelend \DIFaddbegin \DIFadd{in }\DIFaddend Figure~\ref{fig:ca_time}\DIFdelbegin \DIFdel{)}\DIFdelend .
	After fine tuning, Softplus significantly reduced the mismatch since the randomness among the three trials shrinks to a range similar to other experiments.
	Fine tuning also improved its classification accuracy and the response latency.
	Notice that all of the networks trained by three different activation functions showed a very similar stabilisation curve against time, which means they all reached an accuracy close to their best after only 300~ms\DIFdelbegin \DIFdel{of biological real time}\DIFdelend . 

	
	\subsection{Power Consumption}
	Neuromorphic engineering aims to build intelligent machines with low power \DIFdelbegin \DIFdel{consumption.
PAFs can provide an energy usage estimate }\DIFdelend \DIFaddbegin \DIFadd{cost, thus power consumption is an important metric for evaluating their performance~\mbox{%DIFAUXCMD
\cite{liu2016bench}
}%DIFAUXCMD
.
	PAF provides energy estimation }\DIFaddend for SNNs without running the models on real neuromorphic hardware.
	For a single neuron, the energy consumption of the \DIFdelbegin \DIFdel{synatic }\DIFdelend \DIFaddbegin \DIFadd{synaptic }\DIFaddend events it triggers is:
	\begin{equation}
	E_{j} = \lambda_j N_j T E_{syn} = \dfrac{y_j N_j T E_{syn}}{\tau_{syn}}~~,
	\label{equ:energy}
	\end{equation}
	where $\lambda_j$ is the output firing rate, $N_j$ is the number of post-synaptic neurons it connects to, $T$ is the testing time, and $E_{syn}$ is the energy cost for a synaptic event of some specific neuromorphic hardware, for example, about 8~nJ on SpiNNaker~\cite{stromatias2013power}.
	Thus to estimate the whole network, we can sum \DIFdelbegin \DIFdel{all synaptic events for every neuron}\DIFdelend \DIFaddbegin \DIFadd{up all the synaptic events of all the neurons}\DIFaddend :
	\begin{equation}
	\sum_j E_{j} =  \dfrac{T E_{syn}}{\tau_{syn}} \sum_{j}y_j N_j.
	\end{equation}
	Thus, it may cost SpiNNaker 0.064~W, 192~J running for $3,000$~s with synaptic events of $8\times10^6/s$ to classify $10,000$ images (300~ms each) with an accuracy of 98.02\%.
	The best performance reported \DIFdelbegin \DIFdel{, }\DIFdelend using the larger network \DIFdelbegin \DIFdel{, }\DIFdelend may cost SpiNNaker 0.43~W operating synaptic event rate at $5.34\times10^7/s$, consuming 4271.6~J to classify all the images for 1~s each.

	\section{Conclusion and Future Work}
	We presented a generalised SNN training method to train an equivalent ANN and transfer the trained weights back to SNNs.
	This training procedure consists of two simple stages: first, estimate \DIFdelbegin \DIFdel{PAF parameters }\DIFdelend \DIFaddbegin \DIFadd{parameter $p$ for PAF }\DIFaddend using Noisy Softplus, and second, use a PAF version of conventional activation functions for ANN training. % can be generalised to activation units other than Noisy Softplus.
	%The training of a SNN model is exactly the same as ANN training, and 
	The trained weights can be directly used in SNN without any further transformation.
	This method requires the least computation complexity while performing most effectively among existing training algorithms.
	% and even more straight-forward than the other ANN offline training methods which requires an extra step of converting ANN-trained weights to SNN's.

	In terms of classification/recognition accuracy, the performance of ANN-trained SNNs is nearly equivalent to ANNs, and the performance loss can be partially solved by fine tuning.
	The best classification accuracy of 98.85\% using LIF neurons in a PyNN simulation outperforms state-of-the-art SNN models of LIF neurons and is very close to the result using IF neurons~\cite{diehl2015fast}.

	The current limitation prohibiting this off-line SNN training method from wide use lies in the lack of supporting tools, which would enable SNN training on popular deep learning platforms. Additionally an automation tool to read platform-dependent trained weights into the PyNN~\cite{davison2008pynn} language.
	Another issue is the parameter calibration on the scaling factor of the PAF, thus numerical analysis is considered for future work to express the factors with biological parameters of a LIF neuron.
	\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Interesting applications have started with collaborations on speech recognition of cochlea generated spikes which has achieved a promising accuracy at the initial test-idea stage.
\DIFdelbegin \DIFdel{A further goal is to implement deep networks fit for ImageNet~\mbox{%DIFAUXCMD
\cite{deng2009imagenet}
}%DIFAUXCMD
tasks, which will also require modelling various structures of deep learning, for example recurrent neural networks.
}\DIFdelend %DIF > 	A further goal is to implement deep networks fit for ImageNet~\cite{deng2009imagenet} tasks, which will also require modelling various structures of deep learning, for example recurrent neural networks.

	\section*{Acknowledgments}
	To be added after reviewing.
	%The work presented in this paper was largely inspired by discussions at the 2015 Workshops on Neuromorphic Cognition Engineering in CapoCaccia.The authors would like to thank the organisers and the sponsors.The authors would also like to thank Patrick Camilleri, Michael Hopkins, and Viv Woods for meaningful discussions and proof-reading the paper.The construction of the SpiNNaker machine was supported by the Engineering and Physical Sciences Research Council (EPSRC grant EP/4015740/1) with additional support from industry partners ARM Ltd and Silistix Ltd.The research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP/2007-2013) / ERC Grant Agreement n. 320689 and from the EU Flagship Human Brain Project (FP7-604102). The research leading to these results has received funding from Natural Science Foundation of Guangdong Province, China (No: 2016A030313713) and also from  Natural Science Foundation of Guangdong Province, China (No: 2014A030310169).

	%The research leading to these results has received funding from the European Research Council 
	%(FP/2007-2013) / ERC Grant Agreement n. 320689 and from the EU Flagship Human Brain Project (FP7-604102). 
	%Yunhua Chen received funding from the Natural Science Foundation of Guangdong Province, China (No: 2016A030313713) and also from  the Natural Science Foundation of Guangdong Province, China (No: 2014A030310169).
	%Qian Liu personally thanks to the funding from the National Natural Science Foundation of China (61662013,U1501252), the Guangxi Natural Science Foundation~(2014GXNSFDA118036), and The High Level of Innovation Team of Colleges and Universities in Guangxi and Outstanding Scholars Program Funding.

	\small

	\bibliographystyle{unsrt}
	\bibliography{ref}
\end{document}