\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{he2015delving}
\citation{neftci2013event}
\citation{buesing2011neural}
\citation{o2016deep}
\citation{Jug_etal_2012}
\citation{Stromatias2015scalable}
\citation{furber2014spinnaker}
\citation{glorot2011deep}
\citation{cao2015spiking,diehl2015fast}
\citation{srivastava2014dropout}
\citation{diehl2016conversion}
\citation{merolla2014million}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{hunsberger2015spiking}
\citation{Gerstner:2002}
\citation{Noisysoftplus}
\citation{Noisysoftplus}
\citation{Noisysoftplus}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Noisy Softplus fits to the response firing rates of LIF neurons.\relax }}{2}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Fig:nsptau1}{{1}{2}{Noisy Softplus fits to the response firing rates of LIF neurons.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}ANN-Trained SNNs}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Extended Noisy Softplus}{2}{subsection.2.1}}
\newlabel{sec:af_model}{{2.1}{2}{Extended Noisy Softplus}{subsection.2.1}{}}
\newlabel{equ:fit}{{1}{2}{Extended Noisy Softplus}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Equivalent Input and Output}{2}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Artificial spiking neuron takes scaled firing rates as input, then transforms weighted sum in some activation unit to its output which can be scaled-up to the firing rate of an output spike train.\relax }}{3}{figure.caption.3}}
\newlabel{Fig:sneuron}{{2}{3}{Artificial spiking neuron takes scaled firing rates as input, then transforms weighted sum in some activation unit to its output which can be scaled-up to the firing rate of an output spike train.\relax }{figure.caption.3}{}}
\newlabel{equ:mi_input}{{2}{3}{Equivalent Input and Output}{equation.2.2}{}}
\newlabel{equ:si_input}{{3}{3}{Equivalent Input and Output}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Layered-up Network}{3}{subsection.2.3}}
\newlabel{subsec:ns_train}{{2.3}{3}{Layered-up Network}{subsection.2.3}{}}
\newlabel{equ:full_act}{{4}{3}{Layered-up Network}{equation.2.4}{}}
\citation{liu2016bench}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Transforming artificial spiking neurons to artificial neurons for SNN modelling. The combined activation links the firing activity of a spiking neuron to the numerical value of ANNs.\relax }}{4}{figure.caption.4}}
\newlabel{Fig:tneuron}{{3}{4}{Transforming artificial spiking neurons to artificial neurons for SNN modelling. The combined activation links the firing activity of a spiking neuron to the numerical value of ANNs.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Fine Tuning}{4}{subsection.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{4}{section.3}}
\newlabel{sec:iconipResult}{{3}{4}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Neural Activity}{4}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Noisy Softplus fits to the neural response firing rate in an SNN simulation. The recorded firing rate of the same kernel convolved with 10 images in SNN simulation, comparing to the prediction of activations of Softplus, ReLU, and Noisy Softplus.\relax }}{5}{figure.caption.5}}
\newlabel{fig:af_compare}{{4}{5}{Noisy Softplus fits to the neural response firing rate in an SNN simulation. The recorded firing rate of the same kernel convolved with 10 images in SNN simulation, comparing to the prediction of activations of Softplus, ReLU, and Noisy Softplus.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Recognition Performance}{5}{subsection.3.2}}
\newlabel{subsec:result_compare}{{3.2}{5}{Recognition Performance}{subsection.3.2}{}}
\citation{diehl2015fast}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparisons of Loss during training using Noisy Softplus, ReLU and Softplus activation functions. Bold lines show the average of three training trials, and the grey colour illustrates the range between the minimum and the maximum values of the trials. \relax }}{6}{figure.caption.6}}
\newlabel{Fig:loss_ns}{{5}{6}{Comparisons of Loss during training using Noisy Softplus, ReLU and Softplus activation functions. Bold lines show the average of three training trials, and the grey colour illustrates the range between the minimum and the maximum values of the trials. \relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Classification accuracy compared among trained weights of Noisy Softplus, ReLU, Softplus on DNN, SNN and fine-tuned SNN.\relax }}{6}{figure.caption.6}}
\newlabel{Fig:result_bar}{{6}{6}{Classification accuracy compared among trained weights of Noisy Softplus, ReLU, Softplus on DNN, SNN and fine-tuned SNN.\relax }{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparisons of classification accuracy (in \%) of ANN-trained convolutional neural models on original DNN, NEST simulated SNN, and SNN with fine-tuned (FT) model.\relax }}{6}{table.caption.7}}
\newlabel{tbl:ns_result}{{1}{6}{Comparisons of classification accuracy (in \%) of ANN-trained convolutional neural models on original DNN, NEST simulated SNN, and SNN with fine-tuned (FT) model.\relax }{table.caption.7}{}}
\citation{stromatias2013power}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The classification accuracy of 3 trials (averaged in bold lines, grey shading shows the range between minimum to maximum) over short response times, with (a) trained weights before fine tuning, and (b) after fine tuning.\relax }}{7}{figure.caption.8}}
\newlabel{fig:ca_time}{{7}{7}{The classification accuracy of 3 trials (averaged in bold lines, grey shading shows the range between minimum to maximum) over short response times, with (a) trained weights before fine tuning, and (b) after fine tuning.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Power Consumption}{7}{subsection.3.3}}
\newlabel{equ:energy}{{7}{7}{Power Consumption}{equation.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussions}{7}{section.4}}
\citation{diehl2015fast}
\bibstyle{unsrt}
\bibdata{ref}
\bibcite{he2015delving}{{1}{}{{}}{{}}}
\bibcite{neftci2013event}{{2}{}{{}}{{}}}
\bibcite{buesing2011neural}{{3}{}{{}}{{}}}
\bibcite{o2016deep}{{4}{}{{}}{{}}}
\bibcite{Jug_etal_2012}{{5}{}{{}}{{}}}
\bibcite{Stromatias2015scalable}{{6}{}{{}}{{}}}
\bibcite{furber2014spinnaker}{{7}{}{{}}{{}}}
\bibcite{glorot2011deep}{{8}{}{{}}{{}}}
\bibcite{cao2015spiking}{{9}{}{{}}{{}}}
\bibcite{diehl2015fast}{{10}{}{{}}{{}}}
\bibcite{srivastava2014dropout}{{11}{}{{}}{{}}}
\bibcite{diehl2016conversion}{{12}{}{{}}{{}}}
\bibcite{merolla2014million}{{13}{}{{}}{{}}}
\bibcite{hunsberger2015spiking}{{14}{}{{}}{{}}}
\bibcite{Gerstner:2002}{{15}{}{{}}{{}}}
\bibcite{Noisysoftplus}{{16}{}{{}}{{}}}
\bibcite{liu2016bench}{{17}{}{{}}{{}}}
\bibcite{stromatias2013power}{{18}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
