\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{deng2009imagenet}
\citation{silver2016mastering}
\citation{silver2016mastering}
\citation{indiveri2009artificial}
\citation{furber2016large}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Jug_etal_2012}
\citation{siegert1951first}
\citation{liu2016noisy}
\citation{hunsberger2015spiking}
\citation{liu2016noisy}
\citation{cao2015spiking,diehl2015fast}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of the processing mechanisms of an artificial and a spiking neuron. (a) An artificial neuron takes numerical values of vector \textbf  {x} as input, performs a weighted summation followed by an activation function $f$. (b) Spike trains flow into a spiking neuron as current influx, trigger linearly summed PSPs through synapses with different synaptic efficacy \textbf  {w}, and the post-synaptic neuron generates output spikes when the membrane potential reaches some threshold.\relax }}{2}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Fig:compare_as}{{1}{2}{Comparison of the processing mechanisms of an artificial and a spiking neuron. (a) An artificial neuron takes numerical values of vector \textbf {x} as input, performs a weighted summation followed by an activation function $f$. (b) Spike trains flow into a spiking neuron as current influx, trigger linearly summed PSPs through synapses with different synaptic efficacy \textbf {w}, and the post-synaptic neuron generates output spikes when the membrane potential reaches some threshold.\relax }{figure.caption.2}{}}
\citation{liu2016noisy}
\citation{davison2008pynn}
\citation{davison2008pynn}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{3}{section.2}}
\newlabel{sec:back}{{2}{3}{Background}{section.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  NSP models the LIF response function. (a) Firing rates measured by simulations on an LIF neuron driven by different input currents and discrete noise levels. Bold lines show the average and the grey colour fills the range between the minimum and the maximum. (b) NSP activates the input $x$ according to different noise levels where $k=0.16$.\relax }}{3}{figure.caption.3}}
\newlabel{fig:ns}{{2}{3}{NSP models the LIF response function. (a) Firing rates measured by simulations on an LIF neuron driven by different input currents and discrete noise levels. Bold lines show the average and the grey colour fills the range between the minimum and the maximum. (b) NSP activates the input $x$ according to different noise levels where $k=0.16$.\relax }{figure.caption.3}{}}
\newlabel{equ:nsp}{{1}{3}{Background}{equation.2.1}{}}
\newlabel{equ:distr}{{2}{3}{Background}{equation.2.2}{}}
\newlabel{equ:logist}{{3}{3}{Background}{equation.2.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Default parameter settings for the current-based LIF neurons used through this paper, for PyNN\nobreakspace  {}\cite  {davison2008pynn} simulations.\relax }}{3}{table.caption.4}}
\newlabel{tbl:pynnConfig}{{1}{3}{Default parameter settings for the current-based LIF neurons used through this paper, for PyNN~\cite {davison2008pynn} simulations.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{3}{section.3}}
\newlabel{sec:meth}{{3}{3}{Methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Mapping NSP to Concrete Physical Units}{3}{subsection.3.1}}
\newlabel{sec:af_model}{{3.1}{3}{Mapping NSP to Concrete Physical Units}{subsection.3.1}{}}
\newlabel{equ:fit}{{4}{3}{Mapping NSP to Concrete Physical Units}{equation.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces NSP fits to the response firing rates of LIF neurons in concrete physical units. Recorded response firing rate of an LIF neuron driven by synaptic current with two synaptic time constants: (a) $\tau _{\textit  {\textrm  {syn}}}$=1\nobreakspace  {}ms and (b) $\tau _{\textit  {\textrm  {syn}}}$=10\nobreakspace  {}ms. Averaged firing rates of simulation trails are shown in bold lines, and the grey colour fills the range between the minimum to maximum of the firing rates. The thin lines are the scaled NSP.\relax }}{4}{figure.caption.5}}
\newlabel{Fig:nsptau1}{{3}{4}{NSP fits to the response firing rates of LIF neurons in concrete physical units. Recorded response firing rate of an LIF neuron driven by synaptic current with two synaptic time constants: (a) $\tau _{\textit {\textrm {syn}}}$=1~ms and (b) $\tau _{\textit {\textrm {syn}}}$=10~ms. Averaged firing rates of simulation trails are shown in bold lines, and the grey colour fills the range between the minimum to maximum of the firing rates. The thin lines are the scaled NSP.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Parametric Activation Functions\nobreakspace  {}(PAFs)}{4}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The PAF links the firing activity of a spiking neuron to the numerical value of ANNs.\relax }}{4}{figure.caption.6}}
\newlabel{Fig:tneuron}{{4}{4}{The PAF links the firing activity of a spiking neuron to the numerical value of ANNs.\relax }{figure.caption.6}{}}
\newlabel{equ:mi_input}{{5}{4}{Parametric Activation Functions~(PAFs)}{equation.3.5}{}}
\newlabel{equ:PAF}{{6}{4}{Parametric Activation Functions~(PAFs)}{equation.3.6}{}}
\citation{lecun1998gradient}
\citation{gewaltig2007nest}
\citation{liu2016bench}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Generalised SNN Training}{5}{subsection.3.3}}
\newlabel{subsec:ns_train}{{3.3}{5}{Generalised SNN Training}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Fine Tuning}{5}{subsection.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{5}{section.4}}
\newlabel{sec:result}{{4}{5}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experiment Description}{5}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Neural Activity}{6}{subsection.4.2}}
\newlabel{subsec:activity}{{4.2}{6}{Neural Activity}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  The recorded firing rate of the convolution of the same kernel with 10 images in SNN simulation, compared to the firing rate prediction by $S \times f(x)$. NSP (a) fits to the neural response firing rate of LIF neurons more closely than ReLU (b) and Softplus (c).\relax }}{6}{figure.caption.7}}
\newlabel{fig:af_compare}{{5}{6}{The recorded firing rate of the convolution of the same kernel with 10 images in SNN simulation, compared to the firing rate prediction by $S \times f(x)$. NSP (a) fits to the neural response firing rate of LIF neurons more closely than ReLU (b) and Softplus (c).\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Learning Performance}{6}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Recognition Performance}{6}{subsection.4.4}}
\newlabel{subsec:result_compare}{{4.4}{6}{Recognition Performance}{subsection.4.4}{}}
\citation{Jug_etal_2012}
\citation{Stromatias2015scalable}
\citation{hunsberger2015spiking}
\citation{diehl2015fast}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparisons of Loss during training using PAF version of NSP, ReLU and Softplus. Bold lines show the average of three training trials, and the grey colour illustrates the range between the minimum and the maximum values of the trials. \relax }}{7}{figure.caption.8}}
\newlabel{Fig:loss_ns}{{6}{7}{Comparisons of Loss during training using PAF version of NSP, ReLU and Softplus. Bold lines show the average of three training trials, and the grey colour illustrates the range between the minimum and the maximum values of the trials. \relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Classification accuracy. The trained weights were tested using the same activation function as training (DNN\_Orig), then tested using NSP (DNN\_NSP) and transferred to SNN test (SNN\_Orig), and finally fine-tuned to be tested using NSP (DNN+FT\_NSP) and on SNN simulation (SNN\_FT). \relax }}{7}{figure.caption.8}}
\newlabel{Fig:result_bar}{{7}{7}{Classification accuracy. The trained weights were tested using the same activation function as training (DNN\_Orig), then tested using NSP (DNN\_NSP) and transferred to SNN test (SNN\_Orig), and finally fine-tuned to be tested using NSP (DNN+FT\_NSP) and on SNN simulation (SNN\_FT). \relax }{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces SNN training methods comparison.\relax }}{7}{table.caption.9}}
\newlabel{tbl:compare}{{2}{7}{SNN training methods comparison.\relax }{table.caption.9}{}}
\citation{furber2016large}
\citation{liu2016bench}
\citation{furber2016large}
\citation{diehl2015fast}
\citation{davison2008pynn}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The classification accuracy of 3 trials (averaged in bold lines, grey shading shows the range between minimum to maximum) over short response times, with (a) trained weights before fine tuning, and (b) after fine tuning.\relax }}{8}{figure.caption.10}}
\newlabel{fig:ca_time}{{8}{8}{The classification accuracy of 3 trials (averaged in bold lines, grey shading shows the range between minimum to maximum) over short response times, with (a) trained weights before fine tuning, and (b) after fine tuning.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Power Consumption}{8}{subsection.4.5}}
\newlabel{equ:energy}{{8}{8}{Power Consumption}{equation.4.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion and Future Work}{8}{section.5}}
\bibstyle{unsrt}
\bibdata{ref}
\bibcite{deng2009imagenet}{{1}{}{{}}{{}}}
\bibcite{silver2016mastering}{{2}{}{{}}{{}}}
\bibcite{indiveri2009artificial}{{3}{}{{}}{{}}}
\bibcite{furber2016large}{{4}{}{{}}{{}}}
\bibcite{Jug_etal_2012}{{5}{}{{}}{{}}}
\bibcite{siegert1951first}{{6}{}{{}}{{}}}
\bibcite{liu2016noisy}{{7}{}{{}}{{}}}
\bibcite{hunsberger2015spiking}{{8}{}{{}}{{}}}
\bibcite{cao2015spiking}{{9}{}{{}}{{}}}
\bibcite{diehl2015fast}{{10}{}{{}}{{}}}
\bibcite{davison2008pynn}{{11}{}{{}}{{}}}
\bibcite{lecun1998gradient}{{12}{}{{}}{{}}}
\bibcite{gewaltig2007nest}{{13}{}{{}}{{}}}
\bibcite{liu2016bench}{{14}{}{{}}{{}}}
\bibcite{Stromatias2015scalable}{{15}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
